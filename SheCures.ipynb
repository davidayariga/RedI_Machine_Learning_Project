{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d885d273",
      "metadata": {
        "id": "d885d273"
      },
      "source": [
        "## **üß© Problem Statement**\n",
        "The challenge is to create a machine learning model. This model will learn from health-related data (features) of women and then predict whether they have a specific type of diabetes, such as Type 2 or gestational diabetes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üõ†Ô∏è Setting Up the Environment**\n",
        "Before we start, we need to make sure we have all the tools (libraries) we need.\n",
        "\n",
        "Installing Libraries\n",
        "We first install or upgrade two important libraries:\n",
        "\n",
        "* scikit-learn: A fundamental library for machine learning in Python.\n",
        "\n",
        "* pycaret: A low-code machine learning library that helps speed up the experiment cycle."
      ],
      "metadata": {
        "id": "CxicDc0_c_OH"
      },
      "id": "CxicDc0_c_OH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn\n"
      ],
      "metadata": {
        "id": "oS6LPdA382mK",
        "collapsed": true
      },
      "id": "oS6LPdA382mK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V22Bx8Jr1cnb"
      },
      "id": "V22Bx8Jr1cnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üì¶ **Importing Libraries**\n",
        "\n",
        "Next, we import all the necessary libraries into our notebook. These libraries help us with tasks like:\n",
        "\n",
        "* Handling data (pandas, numpy)\n",
        "* Creating visualizations (matplotlib.pyplot, seaborn)\n",
        "* Preparing data for the model (KNNImputer, OneHotEncoder, MinMaxScaler, ColumnTransformer, train_test_split, Pipeline, LabelEncoder)\n",
        "* Building and evaluating models (SVC, LogisticRegression, RandomForestClassifier, DecisionTreeClassifier, accuracy_score, classification_report)\n",
        "* Using PyCaret for automated ML (pycaret.classification)\n",
        "*Ignoring unnecessary warnings (warnings)"
      ],
      "metadata": {
        "id": "TGXHZyWTdSG1"
      },
      "id": "TGXHZyWTdSG1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800f6136",
      "metadata": {
        "id": "800f6136"
      },
      "outputs": [],
      "source": [
        "# üì¶ Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üìÇ **Loading and Understanding the Data**\n",
        "\n",
        "Start by loading the datasets. The are three files:\n",
        "\n",
        "* Train.csv: Contains the training data, including features and the target variable (diabetes type).\n",
        "* Test.csv: Contains the test data, which has the same features but not the target variable. We'll use this to make our final predictions.\n",
        "* SampleSubmission.csv: Shows the format we need for our submission file."
      ],
      "metadata": {
        "id": "zwm9HFDtdmOw"
      },
      "id": "zwm9HFDtdmOw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2aaa3eb",
      "metadata": {
        "id": "e2aaa3eb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# üìÇ Load the dataset (update the path as needed)\n",
        "df = pd.read_csv(\"/content/SheCures/Train.csv\")\n",
        "ts = pd.read_csv(\"/content/SheCures/Test.csv\")\n",
        "ss = pd.read_csv(\"/content/SheCures/SampleSubmission.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting to Know the Data**\n",
        "\n",
        "* df.columns: See the names of all the columns\n",
        "\n",
        "* df.info(): Get a summary, including the number of entries, column names, data types, and non-null counts. This helps identify missing values.\n",
        "\n",
        "* df.describe():  Look at basic statistics (count, mean, standard deviation, min, max, etc.) for numerical columns.\n",
        "\n",
        "* Class Distribution: Check how many samples belong to each diabetes type (our target). This helps us see if the dataset is balanced.\n",
        "\n",
        "* Correlation: Visualize how numerical features relate to each other using a heatmap. This can help identify potentially important features or relationships."
      ],
      "metadata": {
        "id": "UBVA06b5eWPN"
      },
      "id": "UBVA06b5eWPN"
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "N6Pm7h59rxqY"
      },
      "id": "N6Pm7h59rxqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76d052b",
      "metadata": {
        "id": "c76d052b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ‚ÑπÔ∏è Dataset information\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b62cff3",
      "metadata": {
        "id": "0b62cff3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# üìä Basic statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üßÆ Class distribution\n",
        "diabetes_type = \"Target\"\n",
        "df[diabetes_type].value_counts()\n"
      ],
      "metadata": {
        "id": "H6qchc1Zswbx",
        "collapsed": true
      },
      "id": "H6qchc1Zswbx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac333d8f",
      "metadata": {
        "id": "ac333d8f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# üîç Correlation heatmap\n",
        "sns.heatmap(df.corr(numeric_only = True),annot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è **Data Preprocessing**\n",
        "\n",
        "Since machine learning models usually require data to be in a specific format, this section covers cleaning and preparing the data.\n",
        "\n",
        "**Identifying Features**\n",
        "\n",
        "Here I separate the features (the inputs for the model) from the target variable (what I want to predict). I also identify which features are numerical (like age or glucose level) and which are categorical (like 'Yes'/'No' or a specific group)."
      ],
      "metadata": {
        "id": "LskY1RzrfbfO"
      },
      "id": "LskY1RzrfbfO"
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"Target\"\n",
        "features = df.drop(columns=[target])\n",
        "\n",
        "num_features = features.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "cat_features = features.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()"
      ],
      "metadata": {
        "id": "4uHJEy9pxj_t"
      },
      "id": "4uHJEy9pxj_t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Preprocessing Pipelines**\n",
        "\n",
        "Here I build \"pipelines\" to handle numerical and categorical features separately and consistently.\n",
        "\n",
        "**Numerical Pipeline:**\n",
        "\n",
        "* *KNNImputer*: Fills in missing numerical values using the K-Nearest Neighbors method.\n",
        "\n",
        "* *MinMaxScaler*: Scales numerical features to a range between 0 and 1. This helps many models perform better.\n",
        "\n",
        "**Categorical Pipeline**:\n",
        "\n",
        "* *OneHotEncoder*: Converts categorical text data into numerical format (0s and 1s) so the model can understand it.\n",
        "\n",
        "Finally, I then combine the numerical and categorical pipelines using *ColumnTransformer*."
      ],
      "metadata": {
        "id": "vCT5LAdXf_RT"
      },
      "id": "vCT5LAdXf_RT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing pipelines\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", KNNImputer(n_neighbors=5)),\n",
        "    (\"scaler\", MinMaxScaler())\n",
        "])\n",
        "\n",
        "cat_transformer = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_transformer, num_features),\n",
        "    (\"cat\", cat_transformer, cat_features)\n",
        "])"
      ],
      "metadata": {
        "id": "4mcSeKqQztdH"
      },
      "id": "4mcSeKqQztdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Data**\n",
        "\n",
        "At this stage, it's crucial to split the training data before applying some preprocessing steps to avoid \"data leakage\" (where information from the test set accidentally influences the training process). The split is training set (80%) and a testing set (20%)."
      ],
      "metadata": {
        "id": "ipN_JNJKg1w-"
      },
      "id": "ipN_JNJKg1w-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data before preprocessing to avoid data leakage\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(df.drop(columns=[target]), df[target], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "G-6qnuOizUll"
      },
      "id": "G-6qnuOizUll",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Encode categorical variables and split features/target as needed\n",
        "df_cleaned = preprocessor.fit_transform(df.drop(columns=[target])) # Drop target column before preprocessing\n",
        "new_columns = (\n",
        "    num_features +\n",
        "    list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))\n",
        ")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "kRpItir72a2-"
      },
      "id": "kRpItir72a2-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Preprocessing**\n",
        "\n",
        "Now I applied the preprocessor to both the training and testing sets. This imputed missing values, scaled numbers and one-hot encoded categories."
      ],
      "metadata": {
        "id": "ks9EkwLGhkLA"
      },
      "id": "ks9EkwLGhkLA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform training and test data\n",
        "X_train = preprocessor.fit_transform(X_train_raw)\n",
        "X_test = preprocessor.transform(X_test_raw)"
      ],
      "metadata": {
        "id": "vCdjohXW0cNn"
      },
      "id": "vCdjohXW0cNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the names of the new columns created by one-hot encoding\n",
        "encoded_cat_columns = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features)\n",
        "new_columns = num_features + list(encoded_cat_columns)\n"
      ],
      "metadata": {
        "id": "Fszfvpgu0hbF"
      },
      "id": "Fszfvpgu0hbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ü§ñ **Building Models with PyCaret**\n",
        "\n",
        "PyCaret simplifies the process of training and comparing many different machine learning models.\n",
        "\n",
        "**Setting Up PyCaret**\n",
        "\n",
        "Here I initialized the PyCaret environment. I provided the training data (before our manual preprocessing, as PyCaret can handle it), specified the target column, and set some parameters. I dropped the 'ID' column as it's not a feature."
      ],
      "metadata": {
        "id": "jrg2_s4jiDdL"
      },
      "id": "jrg2_s4jiDdL"
    },
    {
      "cell_type": "code",
      "source": [
        "df_pycaret = df.drop(columns=[\"ID\"])"
      ],
      "metadata": {
        "id": "zXYvJTTL3C-p"
      },
      "id": "zXYvJTTL3C-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Set up PyCaret\n",
        "from pycaret.classification import *\n",
        "\n",
        "classf = setup(data = df_pycaret, target = 'Target', train_size = 0.8,\n",
        "               normalize = True, session_id = 123)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "L47aaIMaRnnG"
      },
      "id": "L47aaIMaRnnG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing Models**\n",
        "\n",
        "PyCaret can automatically train and evaluate several common classification models and show us which one performs best based on standard metrics."
      ],
      "metadata": {
        "id": "BX7Bnfh7iioQ"
      },
      "id": "BX7Bnfh7iioQ"
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = compare_models()"
      ],
      "metadata": {
        "id": "wX7jk7eT8hqM"
      },
      "id": "wX7jk7eT8hqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Specific Model**\n",
        "\n",
        "Based on the compare_models() results, I created a specific model. Here, I chose Random Forest ('rf') based on the F1 Score."
      ],
      "metadata": {
        "id": "Ts5kY6PhipgL"
      },
      "id": "Ts5kY6PhipgL"
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model('rf')"
      ],
      "metadata": {
        "id": "0MOQau0FCnGp"
      },
      "id": "0MOQau0FCnGp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä **Evaluating the Model**\n",
        "\n",
        "I needed to see how well my chosen model performs, especially on data it hasn't seen before.\n",
        "\n",
        "**Classification Report**\n",
        "\n",
        "Here I used the manually split test set (X_test_raw and y_test) to evaluate the PyCaret model. The classification_report gives us precision, recall, and F1-score for each class. **Note**:  Prediction is done on X_test_raw because PyCaret models expect data in the original format and handle preprocessing internally."
      ],
      "metadata": {
        "id": "C64j6VrWjMk3"
      },
      "id": "C64j6VrWjMk3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the f1 score of the model\n",
        "\n",
        "print(classification_report(y_test, predict_model(model, data=X_test_raw)[\"prediction_label\"]))\n"
      ],
      "metadata": {
        "id": "D3FBEl4FXSBD"
      },
      "id": "D3FBEl4FXSBD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizations**\n",
        "\n",
        "PyCaret provides easy ways to visualize model performance:\n",
        "\n",
        "* *Confusion Matrix*: Shows how many predictions were correct and where the model made mistakes.\n",
        "* *Feature Importance*: Shows which features the model found most important for making predictions."
      ],
      "metadata": {
        "id": "Q8U2vBHKj0ft"
      },
      "id": "Q8U2vBHKj0ft"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(best_model, plot='confusion_matrix')\n",
        "plot_model(best_model, plot='feature')\n"
      ],
      "metadata": {
        "id": "4ETkh2SK-i75"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4ETkh2SK-i75"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÆ **Making Predictions**\n",
        "\n",
        "Now I used the trained model to predict diabetes types for the actual test dataset (ts).\n",
        "\n",
        "**Preparing Test Data and Predicting**\n",
        "\n",
        "First, I needed to prepare the test set (ts) by dropping the 'ID' column. Then, I used predict_model to get the predictions."
      ],
      "metadata": {
        "id": "DutFRA-JkX6c"
      },
      "id": "DutFRA-JkX6c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ID column from test set\n",
        "test_ids = ts[\"ID\"]\n",
        "ts_clean = ts.drop(columns=[\"ID\"], errors='ignore')\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = predict_model(model, data=ts_clean)\n",
        "\n",
        "predictions.head(10)"
      ],
      "metadata": {
        "id": "26pvfeXlh8H-"
      },
      "id": "26pvfeXlh8H-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the model for deployment**"
      ],
      "metadata": {
        "id": "m0usGI5DCh-7"
      },
      "id": "m0usGI5DCh-7"
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(model)\n",
        "\n",
        "save_model(final_model, 'classification_model')"
      ],
      "metadata": {
        "id": "_0uB0tCLiRhh"
      },
      "id": "_0uB0tCLiRhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a submission File for sumitting to the contest**"
      ],
      "metadata": {
        "id": "8OhSHNizC5ej"
      },
      "id": "8OhSHNizC5ej"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"diabetes_type\": final_labels\n",
        "})\n",
        "\n",
        "submission_path = \"/content/SheCures/submission_file.csv\"\n",
        "submission.to_csv(submission_path, index=False)"
      ],
      "metadata": {
        "id": "ZQSRnW3QDDYI"
      },
      "id": "ZQSRnW3QDDYI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}